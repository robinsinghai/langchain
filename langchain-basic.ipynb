{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e768dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27293b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_classic import LLMChain\n",
    "# from langchain.chains import SimpleSequentialChain, SequentialChain\n",
    "from langchain_classic.chains import SimpleSequentialChain, SequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c251407a",
   "metadata": {},
   "source": [
    "OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a18c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmo = OpenAI(model='gpt-3.5-turbo-instruct' ,temperature=0.7)\n",
    "llmo.invoke(\"What would be a good company name for a company that makes colorful socks?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ea4ee5",
   "metadata": {},
   "source": [
    "Prompt Template and LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6448c937",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(input_variables=[\"country\"], \n",
    "                                 template=\"Tell me the capital of {country}\")\n",
    "prompt_template.format(country=\"France\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a144950f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = LLMChain(llm=llmo, prompt=prompt_template)\n",
    "chain.run(\"Germany\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3a9664",
   "metadata": {},
   "source": [
    "Combining Multiple Chains Using simple Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82d8aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_prompt = PromptTemplate(input_variables=['country'],\n",
    "                                template=\"Please tell me the capital of {country}\")\n",
    "capital_chain = LLMChain(llm=llmo, prompt=capital_prompt)\n",
    "famous_template = PromptTemplate(input_variables=['capital'],\n",
    "                                 template='suggest palces to visit in {capital}')\n",
    "famous_chain = LLMChain(llm=llmo, prompt=famous_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc83286",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_seq = SimpleSequentialChain(chains=[capital_chain, famous_chain])\n",
    "chain_seq.run(\"Germany\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f50f32",
   "metadata": {},
   "source": [
    "Sequential Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0681dbfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "capital_prompt = PromptTemplate(input_variables=['country'],\n",
    "                                template=\"Please tell me the capital of {country}\")\n",
    "capital_chain = LLMChain(llm=llmo, prompt=capital_prompt, output_key='capital')\n",
    "\n",
    "famous_template = PromptTemplate(input_variables=['capital'],\n",
    "                                 template='suggest palces to visit in {capital}')\n",
    "famous_chain = LLMChain(llm=llmo, prompt=famous_template, output_key='places')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f406f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_seq = SequentialChain(chains=[capital_chain, famous_chain], \n",
    "                            input_variables=['country'],\n",
    "                            output_variables=['capital', 'places'])\n",
    "chain_seq({\"country\":\"Germany\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a78644",
   "metadata": {},
   "source": [
    "Chatmodels with ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33b1952",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatllm = ChatOpenAI(model='gpt-3.5-turbo' ,temperature=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ceaa068",
   "metadata": {},
   "outputs": [],
   "source": [
    "chatllm([\n",
    "    SystemMessage(content='You are a langchain expert'),\n",
    "    HumanMessage(content='Return only langchain retaled')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fbafa5",
   "metadata": {},
   "source": [
    "Prompt Template + LLM +  Output Parsers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3f24c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Commasepratedoutput(BaseOutputParser):\n",
    "    def parse(self, text: str):\n",
    "        return text.strip().split(\",\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aac03b",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"You are a helpfule assistant. When user give input generate 10 words in a comma seprated list\"\n",
    "human_template = \"{text}\"\n",
    "chatprompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", human_template)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11922dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = chatprompt | chatllm | Commasepratedoutput()\n",
    "chain.invoke({\"text\":\"programming languages\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef1987eb",
   "metadata": {},
   "source": [
    "## Reasoning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a7bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5135cabf",
   "metadata": {},
   "source": [
    "# Langchain Messages "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417f2531",
   "metadata": {},
   "source": [
    "### Message Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621a560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage, HumanMessage, AIMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a assistant.\"),\n",
    "    HumanMessage(content=\"Make a Langchain agent.\"),\n",
    "    AIMessage(content=\"Sure, here is the agent\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0329e36",
   "metadata": {},
   "source": [
    "### Tuple Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3afe50",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    (\"system\",\"You are a assistant.\"),\n",
    "    (\"human\",\"Make a Langchain agent.\"),\n",
    "    (\"ai\",\"Sure, here is the agent\")\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5dd1d2",
   "metadata": {},
   "source": [
    "### Dictionary Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2263364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\":\"You are a assistant.\"},\n",
    "    {\"role\": \"user\", \"content\":\"Make a Langchain agent.\"},\n",
    "    {\"role\": \"assistant\", \"content\":\"Sure, here is the agent\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0283c",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate with Placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed424810",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful {assistant_type}\"),\n",
    "    (\"human\", \"Write a {language} function to calculate {function_name}\")\n",
    "])\n",
    "\n",
    "messages = template.format_messages(\n",
    "    assistant_type=\"programming assistant\",\n",
    "    language=\"Python\",\n",
    "    function_name=\"factorial\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d49aa85",
   "metadata": {},
   "source": [
    "### Message Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f421a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import (\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    "    ChatPromptTemplate\n",
    ")\n",
    "\n",
    "system_template = SystemMessagePromptTemplate.from_template(\n",
    "    \"You are a helpful {assistant_type}\"\n",
    ")\n",
    "human_template = HumanMessagePromptTemplate.from_template(\n",
    "    \"Write a {language} function to calculate {function_name}\"\n",
    ")\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([\n",
    "    system_template, \n",
    "    human_template\n",
    "])\n",
    "\n",
    "messages = chat_prompt.format_prompt(\n",
    "    assistant_type=\"programming assistant\",\n",
    "    language=\"Python\", \n",
    "    function_name=\"factorial\"\n",
    ").to_messages()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d68532",
   "metadata": {},
   "source": [
    "### Using Message History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d202e0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.memory import ChatMessageHistory\n",
    "\n",
    "history = ChatMessageHistory()\n",
    "history.add_user_message(\"Write a Python function to calculate factorial\")\n",
    "history.add_ai_message(\"Here's a Python function for factorial...\")\n",
    "\n",
    "messages = history.messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7216e1",
   "metadata": {},
   "source": [
    "### prompt used in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac62e9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Define once, reuse everywhere\n",
    "question_template = PromptTemplate.from_template( \"Answer this question concisely: {question}\" )\n",
    "question_with_context_template = PromptTemplate.from_template(\"Context information: {context}\\n\\nAnswer this question concisely:{question}\" )\n",
    "# Generate prompts by filling in variables\n",
    "prompt_text = question_template.format(question=\"What is the capitalof France?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf27c07",
   "metadata": {},
   "source": [
    "### chat prompt template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665977b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an English to French translator.\"),\n",
    "    (\"user\", \"Translate this to French: {text}\")\n",
    "])\n",
    "chat = ChatOpenAI()\n",
    "formatted_messages = template.format_messages(text=\"Hello, how are you?\")\n",
    "response = chat.invoke(formatted_messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa2079c",
   "metadata": {},
   "source": [
    "chain = RunnableSequence(first= prompt, middle=[llm], last= output_parser)\n",
    "\n",
    "LCEL also supports adding transformations and custom functions:\n",
    "\n",
    "with_transformation = prompt | llm | (lambda x: x.upper()) |\n",
    "StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebddd200",
   "metadata": {},
   "outputs": [],
   "source": [
    "# branch logic\n",
    "\n",
    "decision_chain = prompt | llm | (lambda x: route_based_on_content(x)) | {\n",
    "    \"summarize\": summarize_chain,\n",
    "    \"analyze\": analyze_chain\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97da44cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to Runnable\n",
    "length_func = lambda x: len(x)\n",
    "\n",
    "chain = prompt | length_func | output_parser\n",
    "\n",
    "# Is converted to:\n",
    "chain = prompt | RunnableLambda(length_func) | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ecfebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Create components\n",
    "prompt = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "llm = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "# Chain them together using LCEL\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "#Execute the workflow with a single call\n",
    "result = chain.invoke({\"topic\": \"programming\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8545d822",
   "metadata": {},
   "source": [
    "### Complex chain example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a635c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Initialize the model\n",
    "llm = GoogleGenerativeAI(model=\"Gemini 2.5 Pro\t\")\n",
    "\n",
    "# First chain generates a story\n",
    "story_prompt = PromptTemplate.from_template(\"Write a short story about{topic}\")\n",
    "story_chain = story_prompt | llm | StrOutputParser()\n",
    "\n",
    "# Second chain analyzes the story\n",
    "analysis_prompt = PromptTemplate.from_template(\"Analyze the following story's mood:\\n{story}\")\n",
    "analysis_chain = analysis_prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003cd770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine chains\n",
    "story_with_analysis = story_chain | analysis_chain\n",
    "\n",
    "# Run the combined chain\n",
    "story_analysis = story_with_analysis.invoke({\"topic\": \"a rainy day\"})\n",
    "print(\"\\nAnalysis:\", story_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de8d8a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "# Using RunnablePassthrough.assign to preserve data\n",
    "# Add 'story' key with generated content\n",
    "\n",
    "enhanced_chain = RunnablePassthrough.assign(story=story_chain).assign(analysis=analysis_chain)\n",
    "\n",
    "# Execute the chain\n",
    "result = enhanced_chain.invoke({\"topic\": \"a rainy day\"})\n",
    "print(result.keys()) # Output: dict_keys(['topic', 'story', 'analysis'])\n",
    "# dict_keys(['topic', 'story', 'analysis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eb2a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "# Alternative approach using dictionary construction\n",
    "manual_chain = (RunnablePassthrough() |\n",
    "# Pass through input\n",
    "    {\n",
    "        \"story\": story_chain,\n",
    "        # Add story result\n",
    "        \"topic\": itemgetter(\"topic\")\n",
    "# Preserve original topic\n",
    "    } |\n",
    "    RunnablePassthrough().assign(\n",
    "# Add analysis based on story\n",
    "    analysis=analysis_chain\n",
    ")\n",
    ")\n",
    "result = manual_chain.invoke({\"topic\": \"a rainy day\"})\n",
    "print(result.keys())\n",
    "# Output: dict_keys(['story', 'topic', 'analysis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eed3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified dictionary construction\n",
    "simple_dict_chain = story_chain | {\"analysis\": analysis_chain}\n",
    "result = simple_dict_chain.invoke({\"topic\": \"a rainy day\"}) \n",
    "print(result.keys()) # Output: dict_keys(['analysis', 'output'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a210816f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac46ef7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d47898a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7501b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1efb810",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
