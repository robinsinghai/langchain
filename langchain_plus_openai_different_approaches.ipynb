{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42b550a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25faf86",
   "metadata": {},
   "source": [
    "### Reasoning with o3-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b47bd7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could you clarify what kind of radius you’d like to calculate? For example, are you referring to:\n",
      "\n",
      "• The radius of a circle from its diameter, circumference, or area?\n",
      "• The radius of a sphere from its volume or surface area?\n",
      "• Something else entirely (such as a radius in a physics problem)?\n",
      "\n",
      "Here are some common formulas for a circle:\n",
      "\n",
      "1. If you have the diameter (d):\n",
      "  radius r = d / 2\n",
      "\n",
      "2. If you have the circumference (C):\n",
      "  radius r = C / (2π)\n",
      "\n",
      "3. If you have the area (A):\n",
      "  radius r = √(A / π)\n",
      "\n",
      "And for a sphere, if you have the volume (V):\n",
      "  radius r = ∛( (3V) / (4π) )\n",
      "\n",
      "Please let me know which scenario or additional details apply so I can help you further!\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are an experienced programmer and analyst\"),\n",
    "    (\"user\", \"{problem}\")\n",
    "])\n",
    "\n",
    "chat = ChatOpenAI(\n",
    "    model_name=\"o3-mini\",\n",
    "    reasoning_effort=\"high\", # Options: low/medium/high akso effect model response speed\n",
    ")\n",
    "\n",
    "chain = template | chat\n",
    "\n",
    "\n",
    "\n",
    "response = chain.invoke({\"problem\":\"calculate radius\"})\n",
    "\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa24eee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "# For factual, consistent responses\n",
    "factual_llm = OpenAI(temperature=0.1, max_tokens=256)\n",
    "# For creative brainstorming\n",
    "creative_llm = OpenAI(temperature=0.8, top_p=0.95, max_tokens=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920cf53",
   "metadata": {},
   "source": [
    "OpenAI: Known for consistent behavior with temperature in the 0.0-1.0 range\n",
    "\n",
    "Anthropic: May need lower temperature settings to achieve similar creativity levels to other providers\n",
    "\n",
    "Gemini: Supports temperature up to 2.0, allowing for more extreme creativity at higher settings\n",
    "\n",
    "Open-source models: Often require different parameter combinations than commercial APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dee2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.llms import OpenAI\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "template = PromptTemplate.from_template(\"\"\"Answer the question based on the context below. If the question cannot be answered using the information provided answer with \"I don't know\".\n",
    "Context: {context}\n",
    "Question: {question}\n",
    "Answer: \"\"\")\n",
    "\n",
    "model = OpenAI()\n",
    "\n",
    "# `prompt` and `completion` are the results of using template and model once\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing EagleAI OpenAI and Cohere's offerings through the `openai` and `cohere` libraries respectively.\",\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})\n",
    "completion = model.invoke(prompt)\n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e60b324",
   "metadata": {},
   "source": [
    "### OpenAI Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac8c2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.chat_models import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "\n",
    "template = ChatPromptTemplate.from_messages([\n",
    "    ('system', 'Answer the question based on the context below. If the question cannot be answered using the information provided answer with \"I don\\'t know\".'),\n",
    "    ('human', 'Context: {context}'),\n",
    "    ('human', 'Question: {question}'),\n",
    "])\n",
    "\n",
    "\n",
    "model = ChatOpenAI()\n",
    "# `prompt` and `completion` are the results of using template and model once\n",
    "\n",
    "prompt = template.invoke({\n",
    "    \"context\": \"The most recent advancements in NLP are being driven by Large Language Models (LLMs). These models outperform their smaller counterparts and have become invaluable for developers who are creating applications with NLP capabilities. Developers can tap into these models through Hugging Face's `transformers` library, or by utilizing OpenAI and Cohere's offerings through the `openai` and `cohere` libraries respectively.\",\n",
    "    \"question\": \"Which model providers offer LLMs?\"\n",
    "})\n",
    "\n",
    "completion = model.invoke(prompt)\n",
    "\n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3305ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAI\n",
    "model = OpenAI(model='gpt-3.5-turbo-instruct' ,temperature=0.7)\n",
    "prompt = 'The sky is'\n",
    "completion = model.invoke(prompt) \n",
    "print(completion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442ab5c3",
   "metadata": {},
   "source": [
    "### DALL-E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "144fdcfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"https://oaidalleapiprodscus.blob.core.windows.net/private/org-nc2pv8bwnIPshDjmgiOXxq6w/user-jwOqHRMzwHbOBxKgbqVgns66/img-ImMcVeoSDCo20PvmXnqDzyO3.png?st=2025-10-26T04%3A27%3A55Z&se=2025-10-26T06%3A27%3A55Z&sp=r&sv=2024-08-04&sr=b&rscd=inline&rsct=image/png&skoid=ed3ea2f9-5e38-44be-9a1b-7c1e65e4d54f&sktid=a48cca56-e6da-484e-a814-9c849652bcb3&skt=2025-10-25T18%3A28%3A01Z&ske=2025-10-26T18%3A28%3A01Z&sks=b&skv=2024-08-04&sig=szcBAAQL5xIPLlRPVIi2aPO47GCrWgnWeI5xDQBGhYc%3D\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain_community.utilities.dalle_image_generator import DallEAPIWrapper\n",
    "dalle = DallEAPIWrapper(\n",
    "    # model_name=\"dall-e-3\",# Options: \"dall-e-2\" (default) or \"dall-e-3\"\n",
    "    size=\"1024x1024\",# Image dimensions\n",
    "    # quality=\"standard\",# \"standard\" or \"hd\" for DALL-E 3\n",
    "    n=1)\n",
    "# Number of images to generate (only for)\n",
    "# Generate an image\n",
    "image_url = dalle.run(\"A detailed technical diagram of a quantumcomputer\")\n",
    "# Display the image in a notebook\n",
    "from IPython.display import Image, display\n",
    "display(Image(url=image_url))\n",
    "# Or save it locally\n",
    "import requests\n",
    "response = requests.get(image_url)\n",
    "with open(\"generated_library.png\", \"wb\") as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cf6faf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
